{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char RNN: Training a recurrent neural network for character - level language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train a recurrent neural network to perform character-level language modeling on the classic Shakespeare's corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import jax.random as jr\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Using PyTorch's DataLoader for convenience (optional)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from rnn_jax.cells.gated import LongShortTermMemoryCell, LongShortTermMemory\n",
    "from rnn_jax.layers import RNNEncoder, DeepRNN\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we read and preprocess the corpus, which is stored in a single `.txt` file. \n",
    "We index the characters with unique integers, from 0 to $n_{vocab}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the book (Moby Dick)\n",
    "file_path = \"shakespeare.txt\"\n",
    "assert os.path.exists(file_path), f\"File not found: {file_path}\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "# normalize whitespace and remove repeated newlines to avoid token explosion\n",
    "text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\", \" \")\n",
    "# wrap with markers for clarity\n",
    "text = f\"<{text}>\"\n",
    "print(\"Loaded text length:\", len(text))\n",
    "# Build character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for c, i in char_to_index.items()}\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding the whole text to the neural network will be too slow to be effective in training the network.\n",
    "For this reason, we split the text in chunks of 128 characters. We use a sliding window with a stride of 32 to mantain some redundancy. \n",
    "\n",
    "The `BookDataset` class is an utility dataset, created using torch's `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sliding-window sequences for char-level modeling\n",
    "seq_len = 128\n",
    "step = 32\n",
    "encoded = np.array([char_to_index[c] for c in text], dtype=np.int32)\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded) - seq_len - 1, step):\n",
    "    inputs.append(encoded[i : i + seq_len])\n",
    "    targets.append(encoded[i + 1 : i + seq_len + 1])\n",
    "inputs = np.array(inputs, dtype=np.int32)\n",
    "targets = np.array(targets, dtype=np.int32)\n",
    "print(f\"Built {len(inputs):,} sequences\")\n",
    "# downsample for smoke-testing if dataset is too large\n",
    "max_samples = -1\n",
    "if max_samples > 0 and len(inputs) > max_samples:\n",
    "    idxs = np.random.choice(len(inputs), size=max_samples, replace=False)\n",
    "    inputs = inputs[idxs]\n",
    "    targets = targets[idxs]\n",
    "    print(f\"Downsampled dataset to {len(inputs)} samples\")\n",
    "print(\"Dataset shape (inputs, targets):\", inputs.shape, targets.shape)\n",
    "print((\"\").join([index_to_char[i] for i in inputs[0]]))\n",
    "\n",
    "\n",
    "# Torch dataset wrapper (simple)\n",
    "class BookDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.Y = torch.from_numpy(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].long(), self.Y[idx].long()\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "dataset = BookDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(\"Prepared dataloader with batch size\", batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we can showcase how to easily define new function that take in input cells in the library.\n",
    "For example, CharRNN is basically an embedding layer, followed by a (deep) RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(eqx.Module):\n",
    "    embed: eqx.nn.Embedding\n",
    "    encoder: RNNEncoder\n",
    "    decoder: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, *, key):\n",
    "        k0, k1, k2, k3 = jr.split(key, 4)\n",
    "        self.embed = eqx.nn.Embedding(vocab_size, emb_dim, key=k0)\n",
    "        cell = LongShortTermMemoryCell(emb_dim, hidden_dim, key=k1,)\n",
    "\n",
    "        self.encoder = RNNEncoder(cell=cell, key=k2)\n",
    "        self.decoder = eqx.nn.Linear(hidden_dim, vocab_size, key=k3)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x: (seq_len,) or (seq_len, emb_dim) per-sample\n",
    "        x_emb = eqx.filter_vmap(self.embed)(x)\n",
    "        hidden_all = self.encoder(x_emb)\n",
    "        logits = jax.vmap(self.decoder)(hidden_all)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a very simple model, with a single hidden LSTM layer and 256 neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance (small to allow quick tests)\n",
    "vocab_size = len(chars)\n",
    "emb_dim = 256\n",
    "hidden_dim = 512\n",
    "key = jr.PRNGKey(0)\n",
    "model = CharRNN(vocab_size=vocab_size, emb_dim=emb_dim, hidden_dim=hidden_dim, key=key)\n",
    "print(\"Model created: vocab\", vocab_size, \"emb\", emb_dim, \"hidden\", hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.chain(optax.clip_by_global_norm(1.), optax.rmsprop(1e-3))\n",
    "# apply a schedule\n",
    "\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "@eqx.filter_value_and_grad(has_aux=True)\n",
    "def forward_and_loss(model, X, Y):\n",
    "    logits = eqx.filter_vmap(model)(X)  # (batch, seq_len, vocab)\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, Y))\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update(model, opt_state, X, Y):\n",
    "    (loss, _), grads = forward_and_loss(model, X, Y)\n",
    "    updates, opt_state = optimizer.update(\n",
    "        grads, opt_state, params=eqx.filter(model, eqx.is_inexact_array)\n",
    "    )\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "def train_model(model, opt_state, dataloader, n_epochs=1):\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        bar = tqdm(dataloader)\n",
    "        b_losses = []\n",
    "        for Xb, Yb in bar:\n",
    "            Xa = jnp.array(Xb.numpy()) # Convert Torch tensor to JAX array\n",
    "            Ya = jnp.array(Yb.numpy())\n",
    "            model, opt_state, loss = update(model, opt_state, Xa, Ya)\n",
    "            b_losses.append(float(loss))\n",
    "            bar.set_description(f\"batch loss: {loss:.4f}\")\n",
    "    return model, opt_state, b_losses  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "model, opt_state, b_losses = train_model(model, opt_state, small_dataloader, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(b_losses, marker=\"o\", alpha=0.7)\n",
    "ax.set_xlabel(\"Batch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training Loss over Epochs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed, max_len=100, temperature=1.0):\n",
    "    \"\"\"Generate text given a seed string.\"\"\"\n",
    "    generated = seed\n",
    "    idxs = [char_to_index[c] for c in seed]\n",
    "    state = jnp.zeros((model.encoder.cell.hdim,)), jnp.zeros((model.encoder.cell.hdim,))\n",
    "    # Warm-up with the seed\n",
    "    print(\"---------\")\n",
    "    for c in seed[:-1]:\n",
    "        x = char_to_index[c]\n",
    "\n",
    "\n",
    "        x_emb = model.embed(x)\n",
    "        state, h = model.encoder.cell(x_emb[0], state)\n",
    "    # Generate new characters\n",
    "    for _ in range(max_len):\n",
    "        x = idxs[-1]\n",
    "        x_emb = model.embed(x)\n",
    "        state, h = model.encoder.cell(x_emb, state)\n",
    "        logits = model.decoder(h)\n",
    "        probs = jax.nn.softmax(logits / temperature)\n",
    "        next_idx = jax.random.choice(\n",
    "            jr.PRNGKey(np.random.randint(0, 1_000_000)), vocab_size, p=probs\n",
    "        )\n",
    "        idxs.append(int(next_idx))\n",
    "        generated += index_to_char[int(next_idx)]\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in [0.1, 0.75, 1.0, 1.5]:\n",
    "    print(f\"Generating text with temperature = {temp}\")\n",
    "    text = generate_text(model, seed=\" \", max_len=1000, temperature=temp)\n",
    "    print(text)\n",
    "    print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For low temperature, the model seems to imitate well the original Shakespeare's corpus style. Increasing the temperature too much leads to increasingly worse, non-sense samples. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
